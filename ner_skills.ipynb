{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35690c53",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "In this notebook I will show how to train an Named Entity Recognition (NER) algorithm in order to be able to extract all relevant skills from an employee or applicant. In the next step I want to use the detected skill for the creation of employee and project competence profiles.\n",
    "\n",
    "The training data was genered with ChatGPT by prompting to create Jira stories in the field of Cloud, NLP and Computer Vision. The datasets were then labeled using the annotator: https://tecoholic.github.io/ner-annotator/ \n",
    "As the annotation process is very time consuming I only annotated texts for Cloud, NLP and Computer Vision. The python nlp library spacy will be used to create a custom NER model which is able to detect skill entities from texts (resumes, jira stories, project decsriptions, training courses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6154154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.blank(\"en\")  # load a new spacy model\n",
    "db = DocBin()  \n",
    "\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad3cded",
   "metadata": {},
   "source": [
    "First we will iterate over the folder labeled_entities were all the datasets are located. Each dataset gets then processed so that the data can be consumed by a spacy model and is then stored as \"./training_data_skills.spacy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1695ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 1533.25it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 179/179 [00:00<00:00, 2355.38it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 179/179 [00:00<00:00, 2796.85it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 227/227 [00:00<00:00, 2838.64it/s]\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'C:/Users/SEPA/lanchain_ir2/labeled_entities'  # Replace with the path to your folder\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    f = open(folder_path + '/' + filename)\n",
    "    TRAIN_DATA = json.load(f)\n",
    "\n",
    "    for text, annot in tqdm(TRAIN_DATA['annotations']): # text ist eben text, annot sind die gelabelten annotations\n",
    "        # print(text) # text\n",
    "        # print(annot) # die annotierten entities\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]: # ents sind einfach nur die beiden wörter die er sich aus start und end zusammenbaut\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                print(\"Skipping entity\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "\n",
    "db.to_disk(\"./training_data_skills.spacy\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a667747",
   "metadata": {},
   "source": [
    "Run this command within a terminal to create the config file for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba99a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m spacy init config config.cfg --lang en --pipeline ner --optimize efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60340461",
   "metadata": {},
   "source": [
    "Run this command within a terminal to start the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d977b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m spacy train config.cfg --output ./ --paths.train ./training_data_skills.spacy --paths.dev ./training_data_skills.spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd842c9",
   "metadata": {},
   "source": [
    "Now we will load the best model and will use to detect the skills from a short description of my technical profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b759171d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I have several years of experience with \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NLP\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SKILL</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    MLOps\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SKILL</span>\n",
       "</mark>\n",
       ". I already implemented \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ticket Classification algorithms\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SKILL</span>\n",
       "</mark>\n",
       " with \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BERT\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SKILL</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Named Entity Recognition algorithms\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SKILL</span>\n",
       "</mark>\n",
       " with \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    spaCy\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SKILL</span>\n",
       "</mark>\n",
       " as well as \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Topic Modeling\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SKILL</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Text Clustering methods.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SKILL</span>\n",
       "</mark>\n",
       " Moreover I have worked with \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AWS\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SKILL</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Kubernetes\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SKILL</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Docker.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SKILL</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp_ner = spacy.load(\"C:/Users/SEPA/topic_modeling/model-best\")\n",
    "\n",
    "doc = nlp_ner('''I have several years of experience with NLP and MLOps. I already implemented Ticket Classification algorithms with BERT, Named Entity Recognition algorithms with spaCy as well as Topic Modeling and Text Clustering methods. Moreover I have worked with AWS, Kubernetes and Docker.''')\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc8201",
   "metadata": {},
   "source": [
    "As one can see, the trained NER Algorithms was able to identify all skills from a short description of my technical profile. In the next step we will try to extract the skills from my Resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95ee3a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Statistics', 'Machine Learning', 'Deep Learning', 'AutoML', 'Python', 'R', 'Matlab', 'Data Scientist', 'data science', 'software development', 'operations', 'Data Science', 'NLP', 'MLOps pipeline', 'ticket', 'NLP algorithms', 'AWS services', 'Lambda', 'Sagemaker', 'EC2', 'S3', 'ECR', 'Step Functions', 'docker containers', 'AWS', 'Software Development', 'BMW eSIM backend system', 'new features', 'Java', 'Kubernetes', 'Cloud', 'AWS', 'CI/CD', 'DevOps', 'Git Workflows', 'Operations', 'SQL', 'PL/pgSQL', 'AWS Services', 'NAS methods', 'image classification', 'NLP', 'bioinformatics', 'Deep Learning architecture', 'Python', 'PyTorch', 'NAS methods', 'OSP-NAS', 'CWP-DARTS', 'DEP-DARTS', 'Data Analytics', 'predictive models', 'R', 'Tableau', 'Applied Deep Learning', 'TensorFlow', 'PyTorch', 'SCINet architecture', 'Python', 'PyTorch', 'time series clustering', 'Non-parametric Hidden Semi-Markov Models', 'physmm', 'Python', 'Reinforcement Learning', 'Deep Reinforcement Learning algorithm', 'PPO', 'Python', 'OpenAI', 'Bayesian Optimization', 'Material Science', 'material science', 'R', 'contour plots', 'ablation analysis', 'Python', 'Java', 'R', 'Tableau', 'Matlab', 'Cloud', 'AWS', 'MLOps', 'SQL', 'Deep Learning', 'PyTorch', 'Linux', 'NAS', 'AutoML', 'CI/CD', 'DevOps', 'German', 'English', 'Portuguese', 'French', 'Machine Learning']\n"
     ]
    }
   ],
   "source": [
    "file_path = 'C:/Users/SEPA/lanchain_ir2/CV_Scheppach_text.txt'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "doc = nlp_ner(content)\n",
    "\n",
    "extracted_skills_cv = []\n",
    "for ent in doc.ents:\n",
    "    extracted_skills_cv.append(ent.text)\n",
    "print(extracted_skills_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141e9a71",
   "metadata": {},
   "source": [
    "Voila! We have successfully trained a NER model which is able to detect the skills from a CV or a short Cover Letter. In the next step we can use these extracted skill to create skillprofiles and label the\n",
    "map the applicants to specific skillclusters (MLOps, Cloud ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d11e766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
